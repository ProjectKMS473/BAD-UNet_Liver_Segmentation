{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import sys\n",
    "import pydicom\n",
    "import random\n",
    "import re\n",
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "plt.set_cmap('gray')\n",
    "%matplotlib inline\n",
    "\n",
    "## Seeding \n",
    "seed = 2019\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "tf.seed = seed\n",
    "\n",
    "\n",
    "IMG_DTYPE = np.float32\n",
    "SEG_DTYPE = np.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(*args,**kwargs):\n",
    "    cmap = kwargs.get('cmap', 'gray')\n",
    "    title= kwargs.get('title','')\n",
    "    if len(args)==0:\n",
    "        raise ValueError(\"No images given to imshow\")\n",
    "    elif len(args)==1:\n",
    "        plt.title(title)\n",
    "        plt.imshow(args[0], interpolation='none')\n",
    "    else:\n",
    "        n=len(args)\n",
    "        if type(cmap)==str:\n",
    "            cmap = [cmap]*n\n",
    "        if type(title)==str:\n",
    "            title= [title]*n\n",
    "        plt.figure(figsize=(n*5,10))\n",
    "        for i in range(n):\n",
    "            plt.subplot(1,n,i+1)\n",
    "            plt.title(title[i])\n",
    "            plt.imshow(args[i], cmap[i])\n",
    "    plt.show()\n",
    "    \n",
    "def normalize_image(img):\n",
    "    \"\"\" Normalize image values to [0,1] \"\"\"\n",
    "    min_, max_ = float(np.min(img)), float(np.max(img))\n",
    "    return (img - min_) / (max_ - min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Author IBBM\n",
    "- Date 1/3/2019 (DD/MM/YYYY)\n",
    "- Link https://github.com/IBBM/Cascaded-FCN\n",
    "\"\"\"\n",
    "def step1_preprocess_img_slice(img_slc):\n",
    "    \"\"\"\n",
    "    Preprocesses the image 3d volumes by performing the following :\n",
    "    1- Set pixels with hounsfield value great than 1200, to zero.\n",
    "    2- Clip all hounsfield values to the range [-100, 400]\n",
    "    3- Apply Histogram Equalization\n",
    "    \"\"\"    \n",
    "    img_slc[img_slc>1200] = 0\n",
    "    img_slc   = np.clip(img_slc, -100, 400)\n",
    "    img_slc = normalize_image(img_slc)\n",
    "\n",
    "    \n",
    "    img_slc = img_slc * 255\n",
    "    img_slc = img_slc.astype('uint8')\n",
    "    img_slc = cv2.equalizeHist(img_slc)\n",
    "    img_slc = normalize_image(img_slc)\n",
    "    return img_slc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGen(keras.utils.Sequence):\n",
    "    def __init__(self, ids, path, batch_size=8, image_size=128):\n",
    "        self.ids = ids\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __load__(self, id_name):\n",
    "        patient_id = id_name.split('_')\n",
    "        image_path = os.path.join(self.path,\"patients\", id_name)\n",
    "        mask_path = os.path.join(self.path,\"masks\")\n",
    "        all_masks = os.listdir(mask_path)\n",
    "        dicom_image = pydicom.dcmread(image_path)\n",
    "        image = step1_preprocess_img_slice(dicom_image.pixel_array)\n",
    "        image = normalize_image(image)\n",
    "        image = np.array(Image.fromarray(image).resize([image_size, image_size])).astype(IMG_DTYPE)\n",
    "        \n",
    "        mask = pydicom.dcmread(os.path.join(mask_path,patient_id[0]+'_liver' , id_name)).pixel_array\n",
    "        mask = mask/255.0\n",
    "        mask = np.clip(mask, 0, 1)\n",
    "        mask = np.array(Image.fromarray(mask).resize([image_size, image_size])).astype(IMG_DTYPE)\n",
    "        mask = mask[:, :, np.newaxis]\n",
    "        return image, mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if(index+1)*self.batch_size > len(self.ids):\n",
    "            self.batch_size = len(self.ids) - index*self.batch_size\n",
    "        \n",
    "        files_batch = self.ids[index*self.batch_size : (index+1)*self.batch_size]\n",
    "\n",
    "        image = []\n",
    "        mask  = []\n",
    "    \n",
    "        for id_name in files_batch:\n",
    "            _img, _mask = self.__load__(id_name)\n",
    "            _img = np.stack((_img,)*3, axis=-1)\n",
    "            image.append(_img)\n",
    "            mask.append(_mask)\n",
    "        \n",
    "        image = np.array(image)\n",
    "        mask  = np.array(mask)\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ids)/float(self.batch_size)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "train_path = \"train\"\n",
    "batch_size = 8\n",
    "epochs = 20\n",
    "\n",
    "## Training Ids\n",
    "images = []\n",
    "for file in os.listdir(os.path.join(train_path, \"patients\")):\n",
    "        images.append(file)\n",
    "print(len(images))\n",
    "\n",
    "val_data_size = len(images)//5\n",
    "\n",
    "valid_ids = images[:val_data_size]\n",
    "train_ids = images[val_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = DataGen(train_ids, train_path, batch_size=batch_size, image_size=image_size)\n",
    "x, y = gen.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = random.randint(0, len(x)-1)\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(x[r])\n",
    "print(r)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(y[r], (image_size, image_size)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UNet model\n",
    "def unet_model():\n",
    "    inputs = Input(x[r].shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    # Bottle Neck\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
    "    \n",
    "    # Decoder\n",
    "    upconv1 = Conv2DTranspose(512, 3, strides=(2, 2), padding='same')(conv5)\n",
    "    concat1 = Concatenate()([upconv1, conv4])\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(concat1)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    upconv2 = Conv2DTranspose(256, 3, strides=(2, 2), padding='same')(conv6)\n",
    "    concat2 = Concatenate()([upconv2, conv3])\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(concat2)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    upconv3 = Conv2DTranspose(128, 3, strides=(2, 2), padding='same')(conv7)\n",
    "    concat3 = Concatenate()([upconv3, conv2])\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(concat3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    upconv4 = Conv2DTranspose(64, 3, strides=(2, 2), padding='same')(conv8)\n",
    "    concat4 = Concatenate()([upconv4, conv1])\n",
    "    conv9 = Conv2D(64, 3, activation='relu')(concat4)\n",
    "    conv9 = Conv2D(64, 3, activation='relu')(conv9)\n",
    "    \n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Define the BADNet model\n",
    "def badnet_model():\n",
    "    inputs = Input(x[r].shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    # Bottle Neck\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=[conv1, conv2, conv3, conv4, conv5])\n",
    "\n",
    "# Combine the UNet and BADNet models\n",
    "def combined_model():\n",
    "    unet = unet_model()\n",
    "    badnet = badnet_model()\n",
    "    \n",
    "    # Freeze the weights of the BADNet model\n",
    "    for layer in badnet.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Get the outputs of the BADNet model at each encoder block\n",
    "    badnet_outputs = badnet(unet.inputs)\n",
    "    \n",
    "    # Pass the outputs through the skip connections to the decoder blocks of UNet\n",
    "    upconv1 = Conv2DTranspose(512, 3, strides=(2, 2), padding='same')(badnet_outputs[4])\n",
    "    concat1 = Concatenate()([upconv1, unet.layers[10].output])\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(concat1)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "          \n",
    "    upconv2 = Conv2DTranspose(256, 3, strides=(2, 2), padding='same')(conv6)\n",
    "    concat2 = Concatenate()([upconv2,unet.layers[8].output])\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(concat2)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    upconv3 = Conv2DTranspose(128, 3, strides=(2, 2), padding='same')(conv7)\n",
    "    concat3 = Concatenate()([upconv3, unet.layers[4].output])\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(concat3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    upconv4 = Conv2DTranspose(64, 3, strides=(2, 2), padding='same')(conv8)\n",
    "    concat4 = Concatenate()([upconv4, unet.layers[2].output])\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(concat4)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "    \n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "    \n",
    "    return Model(inputs=unet.inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=combined_model()\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = DataGen(train_ids, train_path, batch_size=batch_size, image_size=image_size)\n",
    "x, y = gen.__getitem__(0)\n",
    "\n",
    "train_gen = DataGen(train_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
    "valid_gen = DataGen(valid_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
    "\n",
    "train_steps = len(train_ids) // batch_size\n",
    "valid_steps = len(valid_ids) // batch_size\n",
    "\n",
    "model.fit(train_gen, validation_data=valid_gen, steps_per_epoch=train_steps, validation_steps=valid_steps, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/liver_model4.h5')\n",
    "model.save_weights(\"models/liver_weights4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('models/liver_model4.h5', compile= False)\n",
    "valid_gen = DataGen(valid_ids, train_path, image_size=image_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = valid_gen.__getitem__(7)\n",
    "result = model.predict(x)\n",
    "\n",
    "result = result > 0.5\n",
    "print(x.shape, result.shape)\n",
    "\n",
    "imshow(x[1])\n",
    "imshow(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(np.reshape(y[1]*255, (image_size, image_size)), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(result[1]*255, (image_size, image_size)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5, 1):\n",
    "    ## Dataset for prediction\n",
    "    x, y = valid_gen.__getitem__(i)\n",
    "    result = model.predict(x)\n",
    "    result = result > 0.4\n",
    "    \n",
    "    for i in range(len(result)):\n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "        fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "        ax = fig.add_subplot(1, 2, 1)\n",
    "        ax.imshow(np.reshape(y[i]*255, (image_size, image_size)), cmap=\"gray\")\n",
    "#         ax.imshow(x[i])\n",
    "        ax = fig.add_subplot(1, 2, 2)\n",
    "        ax.imshow(np.reshape(result[i]*255, (image_size, image_size)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "random_batch = random.randint(0, len(valid_ids)//batch_size - 1)\n",
    "random_sample = random.randint(0, batch_size-1)\n",
    "random_batch = 4\n",
    "random_sample = 2\n",
    "print(random_batch)\n",
    "print(random_sample)\n",
    "x, y = valid_gen.__getitem__(random_batch)\n",
    "result =  model.predict(x)\n",
    "result = result > 0\n",
    "\n",
    "fig2 = plt.figure(figsize=(6,6))\n",
    "ax2 = fig2.add_subplot(1,1,1)\n",
    "ax2.imshow(x[random_sample])\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(np.reshape(y[random_sample]*255, (image_size, image_size)), cmap=\"gray\")\n",
    "#         ax.imshow(y[i])\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(result[random_sample]*255, (image_size, image_size)), cmap=\"gray\")\n",
    "\n",
    "# imshow(np.reshape(y[random_sample]*255, (image_size, image_size)), cmap=\"gray\")\n",
    "cm_2d = confusion_matrix(y[random_sample].flatten(), result[random_sample].flatten())\n",
    "cm = cm_2d.ravel()\n",
    "\n",
    "# (tn, fp, fn, tp)\n",
    "print(cm_2d)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = Flatten()\n",
    "    y_true_flatten = y_true_f(y_true)\n",
    "    y_pred_f = Flatten()\n",
    "    y_pred_flatten = y_pred_f(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true_flatten * y_pred_flatten)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_flatten) + tf.reduce_sum(y_pred_flatten) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pixel Accuracy \" + str(((cm[3]+cm[0])/(cm[3]+cm[0]+cm[1]+cm[2])*100))+'%' )\n",
    "print(\"True Positive Accuracy \" + str(((cm[3])/(cm[3]+cm[2])*100))+'%' )\n",
    "# print(\"Dice Coefficient \" + str(dice_coef(y[random_sample], result[random_sample])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax= plt.subplot()\n",
    "cm_2d = cm_2d.astype('float') / cm_2d.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_2d, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# print(count_cms)\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['No Tumor', 'Tumor']); ax.yaxis.set_ticklabels(['No Tumor', 'Tumor']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
